{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ali-mohammadi-design/TensorFlow/blob/main/NLP_Uplodaing_text_Encoding_Decoding_making_batches_for_training_models_Text_Generation_Model_uploading_TF_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11c246ba",
      "metadata": {
        "id": "11c246ba"
      },
      "source": [
        "Note: The NLP is like RNN we would use the previous sequential data to predict the next data.\n",
        "We want to input a large data (including only natural language which is words with the specific sequence) and input a sequence of the words to the machine and ask it to predict the next word!\n",
        "\n",
        "At first we would assign some number to each character and then we would train the model to work with the numbers\n",
        "\n",
        "Like other RNN models we also have input layers, hidden layers and output layers!!\n",
        "\n",
        "For creating a model we have three layers:\n",
        "1- Embedding layers:The layer turns integers to the dense vectorsof fixed sizes. ex: [[4],[30]]---> [[0.25,01,03],[0.6,-0.2,0.9]]\n",
        "\n",
        "2-Gated Recurrent Unit (GRU): This layer is like Long Short Memory (LSTM)\n",
        "\n",
        "3- Dense layer: we would have one neuron per character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a5070706",
      "metadata": {
        "id": "a5070706"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d39edc99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d39edc99",
        "outputId": "edb9d924-e5a3-4009-cbe2-bccf825d254f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15d71f2d",
      "metadata": {
        "id": "15d71f2d"
      },
      "source": [
        "Here we want to import a shakspier text as a data and train our model based on this sequential data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "acadf66a",
      "metadata": {
        "id": "acadf66a"
      },
      "outputs": [],
      "source": [
        "path_to_file=\"/content/shakespeare.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1a981c68",
      "metadata": {
        "id": "1a981c68"
      },
      "outputs": [],
      "source": [
        "text=open(path_to_file,'r').read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "906265ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "906265ef",
        "outputId": "5ae131b8-f38b-4cae-d0ac-5890ccab1238"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5445609"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "98d28a3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98d28a3c",
        "outputId": "f6aa5e10-7be8-4f2f-eec1-e19e97323def"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "type(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "036d6902",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "036d6902",
        "outputId": "3d7fd6cc-9216-4db2-84bc-f411a8eccbf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "et.\n",
            "\n",
            "\n",
            "                     6  \n",
            "  Then let not winter's ragged hand deface,\n",
            "  In thee thy summer ere thou be distilled:\n",
            "  Make sweet some vial; treasure thou some place,\n",
            "  With beauty's treasure ere it be self-killed:\n",
            "  That use is not forbidden usury,\n",
            "  Which happies those that pay the willing loan;\n",
            "  That's for thy self to breed another thee,\n",
            "  Or ten times happier be it ten for one,\n",
            "  Ten times thy self were happier than thou art,\n",
            "  If ten of thine ten times refigured thee:\n",
            "  Then what could death do if thou shouldst depart,\n",
            "  Leaving thee living in posterity?\n",
            "    Be not self-willed for thou art much too fair,\n",
            "    To be death's conquest and make worms thin\n"
          ]
        }
      ],
      "source": [
        "print(text[3334:4000])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e129cc",
      "metadata": {
        "id": "24e129cc"
      },
      "source": [
        "Note: We can use the set function to see the objects in a list:\n",
        "The set() function creates a set object. The items in a set list are unordered, so it will appear in random order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ecc6b528",
      "metadata": {
        "id": "ecc6b528"
      },
      "outputs": [],
      "source": [
        "#set(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4da68cb4",
      "metadata": {
        "id": "4da68cb4"
      },
      "outputs": [],
      "source": [
        "vocab=sorted(set(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2c000052",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c000052",
        "outputId": "a3894f32-7c14-4a04-fdd6-1769ac750f5b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "type(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2f02e7c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f02e7c8",
        "outputId": "573d1578-4069-4e00-d793-0d6be2a9e859"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3dde135",
      "metadata": {
        "id": "a3dde135"
      },
      "source": [
        "Len of vocab is 84. Thus we need 84 in our last layer which is dense layer!!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb523947",
      "metadata": {
        "id": "bb523947"
      },
      "source": [
        "**Encoding Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f416f766",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "f416f766",
        "outputId": "3cdf1c69-bf90-4778-d6b4-03cafed4913f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'M'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "vocab[38]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0e08e4d",
      "metadata": {
        "id": "e0e08e4d"
      },
      "source": [
        "Note: The enumerate function in Python is a built-in function that allows you to loop over a list (or any other iterable) and have an automatic counter. It is commonly used when you need a counter with the elements of the list. Here is the basic syntax:\n",
        "\n",
        "enumerate(iterable, start=0)\n",
        "\n",
        "iterable: The iterable you want to loop over.\n",
        "start: The value from which the counter should start (default is 0).\n",
        "\n",
        "Ex: Imagine you have a list of fruits:\n",
        "fruits = ['apple', 'banana', 'cherry', 'date']\n",
        "Without enumerate, if you want to print each fruit with its index, you might do something like this:\n",
        "\n",
        "index = 0\n",
        "for fruit in fruits:\n",
        "    print(index, fruit)\n",
        "    index += 1\n",
        "    \n",
        "Output:\n",
        "0 apple\n",
        "1 banana\n",
        "2 cherry\n",
        "3 date\n",
        "\n",
        "With enumerate, you can simplify this:\n",
        "\n",
        "for index, fruit in enumerate(fruits):\n",
        "    print(index, fruit)\n",
        "\n",
        "Output:\n",
        "0 apple\n",
        "1 banana\n",
        "2 cherry\n",
        "3 date\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a90a18f5",
      "metadata": {
        "id": "a90a18f5"
      },
      "outputs": [],
      "source": [
        "#for pair in enumerate(vocab):\n",
        "#    print(pair)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf7e2de0",
      "metadata": {
        "id": "cf7e2de0"
      },
      "source": [
        "we can use enumerate to generate a dictionary that can map  each word to a number!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cc4edc91",
      "metadata": {
        "id": "cc4edc91"
      },
      "outputs": [],
      "source": [
        "char_to_index={char:ind for ind, char in enumerate(vocab)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b18d8294",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b18d8294",
        "outputId": "f7cd2338-0b42-45b3-b733-e93c24932f55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "type(char_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9eb17082",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eb17082",
        "outputId": "47407557-9db9-43ea-d98c-367ee67b9c15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "char_to_index['c']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0f481e96",
      "metadata": {
        "id": "0f481e96"
      },
      "outputs": [],
      "source": [
        "#char_to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2039650e",
      "metadata": {
        "id": "2039650e"
      },
      "outputs": [],
      "source": [
        "index_to_char=np.array(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "27baf70e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27baf70e",
        "outputId": "b74baeac-1c5b-4ed2-d8ca-2c0dee2f0833"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1',\n",
              "       '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?',\n",
              "       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
              "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
              "       '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i',\n",
              "       'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v',\n",
              "       'w', 'x', 'y', 'z', '|', '}'], dtype='<U1')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "index_to_char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "db31ad0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "db31ad0d",
        "outputId": "ce439461-da79-4cb7-a930-604c4e122a5b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'T'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "index_to_char[45]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9436c63b",
      "metadata": {
        "id": "9436c63b"
      },
      "source": [
        "Now we can encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "5d877d9d",
      "metadata": {
        "id": "5d877d9d"
      },
      "outputs": [],
      "source": [
        "encoded_text=np.array([char_to_index[c] for c in text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "be15f51e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be15f51e",
        "outputId": "c912e0b0-0093-4aa1-c652-2f29e2ce731e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1, 12,  0,  1,  1, 31, 73, 70, 68,  1, 61, 56, 64,\n",
              "       73, 60, 74, 75,  1, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "encoded_text[:40]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "f3c1f044",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "f3c1f044",
        "outputId": "807307c0-d9b0-4199-f1e5-acc62930cc64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n                     1\\n  From fairest c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "text[:40]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5a966eec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a966eec",
        "outputId": "338a4979-5608-45e0-a710-a091126d3243"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5445609,)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "encoded_text.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "8f60da51",
      "metadata": {
        "id": "8f60da51"
      },
      "outputs": [],
      "source": [
        "decoded_text= [index_to_char[c] for c in encoded_text[:40]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "2aa1042e",
      "metadata": {
        "id": "2aa1042e"
      },
      "outputs": [],
      "source": [
        "#decoded_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d52fd8b1",
      "metadata": {
        "id": "d52fd8b1"
      },
      "source": [
        "Creating Bathes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7ee2c9e",
      "metadata": {
        "id": "a7ee2c9e"
      },
      "source": [
        "Note: We need to make batched of the data that for each batch we have only one letter different with the another one. Obviously the computer would work with the numbers. Thus we just have one number less or more for each batches with the specific sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "587a1715",
      "metadata": {
        "id": "587a1715"
      },
      "source": [
        "Note: We have to determine the len of each batch large enough to get the main structure of the batch. each line has about 40 letter. Three lines usually have specific structure of the batch thus 120 letters seems good size for each batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ac2a17f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac2a17f1",
        "outputId": "7850b71d-b6ac-42ef-d990-93d8adb6ec30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "et.\n",
            "\n",
            "\n",
            "                     6  \n",
            "  Then let not winter's ragged hand deface,\n",
            "  In thee thy summer ere thou be distilled:\n",
            "  Make sweet some vial; treasure thou some place,\n",
            "  With beauty's treasure ere it be self-killed:\n",
            "  That use is not forbidden usury,\n",
            "  Which happies those that pay the willing loan;\n",
            "  That's for thy self to breed another thee,\n",
            "  Or ten times happier be it ten for one,\n",
            "  Ten times thy self were happier than thou art,\n",
            "  If ten of thine ten times refigured thee:\n",
            "  Then what could death do if thou shouldst depart,\n",
            "  Leaving thee living in posterity?\n",
            "    Be not self-willed for thou art much too fair,\n",
            "    To be death's conquest and make worms thin\n"
          ]
        }
      ],
      "source": [
        "print(text[3334:4000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "e1f59da7",
      "metadata": {
        "id": "e1f59da7"
      },
      "outputs": [],
      "source": [
        "line=\"Then let not winter's ragged hand deface,\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "7da60038",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7da60038",
        "outputId": "b81eaea9-37df-4051-f903-b1f45333e6b1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "len(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "5313399f",
      "metadata": {
        "id": "5313399f"
      },
      "outputs": [],
      "source": [
        "seq_len=120"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "08415c9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08415c9e",
        "outputId": "f5b969e0-2ed5-4768-a520-14a51c58c930"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45005.03305785124"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "len(text)/(seq_len+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "fdc21dab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdc21dab",
        "outputId": "406d2445-252c-4056-b69e-0a0c91dfaa15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45005"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "total_num_seq=len(text)//(seq_len+1)\n",
        "total_num_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1960983",
      "metadata": {
        "id": "c1960983"
      },
      "source": [
        "Making training sequences with tensorfelow.\n",
        "\n",
        "Note: encoded_text is a list with the following code we would convert it to a dataset.\n",
        "\n",
        "When you use tf.data.Dataset directly with a list or array, you typically use methods like from_tensors or from_tensor_slices. Here’s a quick comparison:\n",
        "\n",
        "tf.data.Dataset.from_tensors: This method takes a single tensor and creates a dataset with a single element (the entire tensor).\n",
        "\n",
        "tf.data.Dataset.from_tensor_slices\n",
        "When you use tf.data.Dataset.from_tensor_slices, each element of the list becomes a separate element in the dataset.\n",
        "\n",
        "tf.data.Dataset.from_tensor_slices: This method takes a list or array and creates a dataset where each element of the list or array is a separate element in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "3fb8ac09",
      "metadata": {
        "id": "3fb8ac09"
      },
      "outputs": [],
      "source": [
        "char_data_set=tf.data.Dataset.from_tensor_slices(encoded_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "3e2ceeeb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "3e2ceeeb",
        "outputId": "187bdbfe-3358-43aa-ba82-9d7f0f1b03e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.from_tensor_slices_op._TensorSliceDataset"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>tensorflow.python.data.ops.from_tensor_slices_op._TensorSliceDataset</b><br/>def __init__(element, is_files=False, name=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_tensor_slices_op.py</a>A `Dataset` of slices from a dataset element.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 28);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "type(char_data_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ace5eb6",
      "metadata": {
        "id": "4ace5eb6"
      },
      "source": [
        "Now we nned to make this slices as a sequences of some batches that have some shuffle!\n",
        "Please look at the following code to understand:\n",
        "\n",
        "Note: .take(500): This method is called on the dataset to create a new dataset containing only the first 500 elements from char_data_set. It does not modify the original dataset but creates a new one that limits the number of elements to the specified count (500 in this case)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "b7c8f550",
      "metadata": {
        "id": "b7c8f550"
      },
      "outputs": [],
      "source": [
        "#for item in char_data_set.take(500):\n",
        "#    print(index_to_char[item.numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "388cc30b",
      "metadata": {
        "id": "388cc30b"
      },
      "outputs": [],
      "source": [
        "sequence=char_data_set.batch(seq_len+1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eab0969e",
      "metadata": {
        "id": "eab0969e"
      },
      "source": [
        "char_data_set is a dataset now we want to convert it to various batches with the same size."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e0cf2a7",
      "metadata": {
        "id": "5e0cf2a7"
      },
      "source": [
        "Note: drop_remainder is equal to True would mean that the remainder of text would be dropped. for instance we have 45000 batches that each one has 120 characters. if some letters are remained after filling last batch they would be dropped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "3d797e70",
      "metadata": {
        "id": "3d797e70"
      },
      "outputs": [],
      "source": [
        "def creat_seq_targets(seq):\n",
        "    #with this function we would make a shift for each batch\n",
        "    input_text=seq[:-1]\n",
        "    #from zero to the last character\n",
        "    #EX: the main text is \"Hello How are you?\". Input text will be \"Hello How are you\". target_text will be  \"ello How are you?\"\n",
        "    target_text=seq[1:]\n",
        "\n",
        "    return input_text, target_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "a6bd6851",
      "metadata": {
        "id": "a6bd6851"
      },
      "outputs": [],
      "source": [
        "# to better understand:\n",
        "line=\"Then let not winter's ragged hand deface,\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "22b95a55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "22b95a55",
        "outputId": "d264393b-c00a-486f-a8b4-3fd0ba13d7e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Then let not winter's ragged hand deface\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "line[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "7e4d7baa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7e4d7baa",
        "outputId": "e07dd69a-ca68-4444-ba85-94b89215f2f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"hen let not winter's ragged hand deface,\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "line[1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "088ad3c9",
      "metadata": {
        "id": "088ad3c9"
      },
      "source": [
        "Now we can map it to the sequences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "d56b9cf0",
      "metadata": {
        "id": "d56b9cf0"
      },
      "outputs": [],
      "source": [
        "dataset=sequence.map(creat_seq_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "11d0d934",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11d0d934",
        "outputId": "b4aefdb5-7cda-4d95-c45d-ea398b2c3777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0\n",
            "  1  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74\n",
            "  1 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45\n",
            " 63 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74\n",
            " 60  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75]\n",
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But\n",
            "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0  1\n",
            "  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74  1\n",
            " 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45 63\n",
            " 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74 60\n",
            "  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75  1]\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But \n"
          ]
        }
      ],
      "source": [
        "for input_text, target_text in dataset.take(1):\n",
        "    print(input_text.numpy())\n",
        "    print(\"\".join(index_to_char[input_text.numpy()]))\n",
        "    print(target_text.numpy())\n",
        "    print(\"\".join(index_to_char[target_text.numpy()]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a330f09a",
      "metadata": {
        "id": "a330f09a"
      },
      "source": [
        "As you can see one 1 has been added to the target and in the sentence also a blank \"\" has been added!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "db81cd4a",
      "metadata": {
        "id": "db81cd4a"
      },
      "outputs": [],
      "source": [
        "batch_size= 128\n",
        "#we want to have 128 batches that each batch gas 120 letter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "2025ee10",
      "metadata": {
        "id": "2025ee10"
      },
      "outputs": [],
      "source": [
        "buffer_size=1000\n",
        "dataset=dataset.shuffle(buffer_size).batch(batch_size,drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54c1b4f4",
      "metadata": {
        "id": "54c1b4f4"
      },
      "source": [
        "Note:\n",
        "\n",
        "Shuffle: Randomizes the order of the elements in the dataset using a buffer of size buffer_size.\n",
        "Batch: Groups the shuffled elements into batches of size batch_size."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66a568c7",
      "metadata": {
        "id": "66a568c7"
      },
      "source": [
        "Buffer Size: It defines how many elements from the dataset are kept in memory at any one time for shuffling. This allows us to shuffle the data in smaller chunks rather than trying to shuffle the entire dataset at once.\n",
        "How It Works\n",
        "Loading Data into Buffer: The buffer is like a small storage area where a subset of the dataset is kept.\n",
        "Shuffling within the Buffer: The elements in the buffer are shuffled, and elements are drawn from this buffer.\n",
        "Refilling the Buffer: As elements are taken out of the buffer for processing, new elements from the dataset are added to the buffer to maintain its size until all elements are processed.\n",
        "Example\n",
        "Let's illustrate this with a simple example.\n",
        "\n",
        "Dataset\n",
        "Imagine you have a dataset of 10 elements: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].\n",
        "\n",
        "Buffer Size = 3\n",
        "Initial Buffer Fill:\n",
        "Load the first 3 elements into the buffer: [1, 2, 3].\n",
        "Shuffle the Buffer:\n",
        "Let's say the buffer is shuffled to: [3, 1, 2].\n",
        "Draw an Element from the Buffer:\n",
        "Draw the first element (3) from the buffer for processing.\n",
        "The buffer now has: [1, 2].\n",
        "Refill the Buffer:\n",
        "Add the next element from the dataset (4) to the buffer: [1, 2, 4].\n",
        "Shuffle the buffer again: [2, 4, 1].\n",
        "Repeat the Process:\n",
        "Continue drawing elements, refilling, and shuffling until the entire dataset is processed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e0be059",
      "metadata": {
        "id": "0e0be059"
      },
      "source": [
        "What is Batch Size?\n",
        "Batch size is a term commonly used in machine learning and data processing. It refers to the number of samples (data points) that are processed together in a single batch during training or evaluation.\n",
        "\n",
        "Why Use Batch Size?\n",
        "Efficiency: Processing data in batches can be more efficient than processing one sample at a time. It allows for parallel processing and better utilization of computational resources.\n",
        "Stability: Using batches can make the training process more stable and help in converging to the optimal solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "a5117917",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5117917",
        "outputId": "c3c01e9a-bcfd-43d2-f245-456107fc2054"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(128, 120), dtype=tf.int64, name=None), TensorSpec(shape=(128, 120), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a3cd21b",
      "metadata": {
        "id": "0a3cd21b"
      },
      "source": [
        "**Creating Model**\n",
        "\n",
        "For the model we need three layers:\n",
        "Embedding layer\n",
        "\n",
        "GRU\n",
        "\n",
        "Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "62663f5f",
      "metadata": {
        "id": "62663f5f"
      },
      "outputs": [],
      "source": [
        "vocab_size=len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "f8baba69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8baba69",
        "outputId": "fdecc443-4b3d-4e32-d618-b21c1423307c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "668a736a",
      "metadata": {
        "id": "668a736a"
      },
      "outputs": [],
      "source": [
        "embedding_dim=64"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b475ccd",
      "metadata": {
        "id": "8b475ccd"
      },
      "source": [
        "NOTE: Embedding dimentions should be more or less in the same range if the vocab size!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "3847259f",
      "metadata": {
        "id": "3847259f"
      },
      "outputs": [],
      "source": [
        "rnn_neurons=1026"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "409deeea",
      "metadata": {
        "id": "409deeea"
      },
      "source": [
        "We have chosen a single layer but lots of neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "eef0aeb5",
      "metadata": {
        "id": "eef0aeb5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.losses import sparse_categorical_crossentropy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db108b6d",
      "metadata": {
        "id": "db108b6d"
      },
      "source": [
        "Sparse categorical cross-entropy is a loss function used in machine learning, specifically in classification problems where you want your model to predict categories or classes. It is particularly useful when you have many categories and your labels are provided as integers instead of one-hot encoded vectors. Let’s break it down with examples to understand it better.\n",
        "\n",
        "Simplified Explanation\n",
        "Cross-Entropy Loss: This is a measure of how well your model's predicted probabilities match the actual labels. It penalizes wrong predictions.\n",
        "\n",
        "Categorical: This means you are dealing with multiple categories or classes (e.g., animals like cats, dogs, birds).\n",
        "\n",
        "Sparse: Instead of using one-hot encoded vectors to represent the labels, we use integer labels. One-hot encoding is a way to represent categories as binary vectors where only one bit is ‘1’ (true) and the rest are ‘0’ (false).\n",
        "\n",
        "Examples\n",
        "One-Hot Encoding vs Sparse Labels\n",
        "One-Hot Encoding:\n",
        "\n",
        "Cat: [1, 0, 0]\n",
        "Dog: [0, 1, 0]\n",
        "Bird: [0, 0, 1]\n",
        "Sparse Labels:\n",
        "\n",
        "Cat: 0\n",
        "Dog: 1\n",
        "Bird: 2\n",
        "Example Scenario\n",
        "Imagine you have a neural network that predicts whether an image is of a cat, dog, or bird.\n",
        "\n",
        "Predicted Probabilities:\n",
        "\n",
        "Image 1: [0.7, 0.2, 0.1] (70% cat, 20% dog, 10% bird)\n",
        "Image 2: [0.1, 0.6, 0.3] (10% cat, 60% dog, 30% bird)\n",
        "Image 3: [0.2, 0.3, 0.5] (20% cat, 30% dog, 50% bird)\n",
        "Actual Labels (Sparse Format):\n",
        "\n",
        "Image 1: Cat (0)\n",
        "Image 2: Dog (1)\n",
        "Image 3: Bird (2)\n",
        "Calculation\n",
        "The sparse categorical cross-entropy loss function compares the predicted probabilities with the actual sparse labels. It calculates the loss for each prediction:\n",
        "\n",
        "Image 1:\n",
        "\n",
        "Actual label: Cat (0)\n",
        "Predicted probability for Cat: 0.7\n",
        "Loss: -log(0.7)\n",
        "Image 2:\n",
        "\n",
        "Actual label: Dog (1)\n",
        "Predicted probability for Dog: 0.6\n",
        "Loss: -log(0.6)\n",
        "Image 3:\n",
        "\n",
        "Actual label: Bird (2)\n",
        "Predicted probability for Bird: 0.5\n",
        "Loss: -log(0.5)\n",
        "The total loss is the average of these individual losses. The logarithm is used to penalize predictions that are further from the actual label more heavily.\n",
        "\n",
        "Why Use Sparse Categorical Cross-Entropy?\n",
        "Efficiency: Sparse labels take up less space and are easier to work with when you have a large number of categories.\n",
        "Convenience: When your labels are already in integer form, you don’t need to convert them to one-hot encoded vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91793c86",
      "metadata": {
        "id": "91793c86"
      },
      "source": [
        "**why do we use this function in NLP?**\n",
        "\n",
        "In Natural Language Processing (NLP), sparse categorical cross-entropy is commonly used for tasks involving classification, such as text classification, sentiment analysis, and language modeling. Here are the reasons why this function is particularly useful in NLP:\n",
        "\n",
        "Handling Large Vocabulary Sizes\n",
        "NLP tasks often involve dealing with a large vocabulary of words or tokens. For example, consider a language model predicting the next word in a sentence. The vocabulary can consist of thousands or even millions of words. Sparse categorical cross-entropy is efficient in handling such large vocabularies without the need to convert labels to one-hot encoded vectors.\n",
        "\n",
        "Efficiency and Memory Usage\n",
        "When dealing with large datasets and vocabulary sizes, one-hot encoding labels can be very memory-intensive. Sparse categorical cross-entropy allows us to use integer labels directly, which saves memory and computational resources.\n",
        "\n",
        "Example: Language Modeling\n",
        "Consider a language model that predicts the next word in a sentence. Suppose the vocabulary consists of 10,000 words. Here's how sparse categorical cross-entropy is applied:\n",
        "\n",
        "Input Sentence: \"The cat sits on the\"\n",
        "\n",
        "Predicted Probabilities (for the next word):\n",
        "\n",
        "\"mat\": 0.4\n",
        "\"floor\": 0.3\n",
        "\"table\": 0.1\n",
        "... (9,997 other words)\n",
        "Actual Label: \"mat\" (which corresponds to the index 123 in the vocabulary)\n",
        "\n",
        "Instead of converting \"mat\" into a one-hot encoded vector of length 10,000 (where only the 123rd position is 1, and the rest are 0), we use the integer label 123 directly.\n",
        "\n",
        "Example: Text Classification\n",
        "For text classification tasks like sentiment analysis, where you classify a piece of text as positive, negative, or neutral, sparse categorical cross-entropy is also used:\n",
        "\n",
        "Text: \"I love this movie!\"\n",
        "\n",
        "Predicted Probabilities:\n",
        "\n",
        "Positive: 0.7\n",
        "Negative: 0.2\n",
        "Neutral: 0.1\n",
        "Actual Label: Positive (which corresponds to the index 0 in the label set)\n",
        "\n",
        "Benefits in NLP\n",
        "Scalability: It efficiently scales to large datasets and vocabularies.\n",
        "Simplicity: Directly using integer labels simplifies the implementation and reduces preprocessing steps.\n",
        "Performance: It often leads to faster training and inference times because of reduced memory usage and computational overhead.\n",
        "Summary\n",
        "Sparse categorical cross-entropy is widely used in NLP because it efficiently handles large vocabularies and reduces memory and computational requirements. It simplifies the processing of labels by using integer representations, making it suitable for various NLP tasks such as text classification and language modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "093a4dfd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "093a4dfd",
        "outputId": "e7d5b3c1-6599-4474-fa3f-18e06b72371c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function sparse_categorical_crossentropy in module keras.src.losses:\n",
            "\n",
            "sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1, ignore_class=None)\n",
            "    Computes the sparse categorical crossentropy loss.\n",
            "    \n",
            "    Standalone usage:\n",
            "    \n",
            "    >>> y_true = [1, 2]\n",
            "    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
            "    >>> loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
            "    >>> assert loss.shape == (2,)\n",
            "    >>> loss.numpy()\n",
            "    array([0.0513, 2.303], dtype=float32)\n",
            "    \n",
            "    >>> y_true = [[[ 0,  2],\n",
            "    ...            [-1, -1]],\n",
            "    ...           [[ 0,  2],\n",
            "    ...            [-1, -1]]]\n",
            "    >>> y_pred = [[[[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]],\n",
            "    ...             [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]]],\n",
            "    ...           [[[1.0, 0.0, 0.0], [0.0, 0.5, 0.5]],\n",
            "    ...            [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]]]]\n",
            "    >>> loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
            "    ...   y_true, y_pred, ignore_class=-1)\n",
            "    >>> loss.numpy()\n",
            "    array([[[2.3841855e-07, 2.3841855e-07],\n",
            "            [0.0000000e+00, 0.0000000e+00]],\n",
            "           [[2.3841855e-07, 6.9314730e-01],\n",
            "            [0.0000000e+00, 0.0000000e+00]]], dtype=float32)\n",
            "    \n",
            "    Args:\n",
            "        y_true: Ground truth values.\n",
            "        y_pred: The predicted values.\n",
            "        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n",
            "            default, we assume that `y_pred` encodes a probability distribution.\n",
            "        axis: Defaults to -1. The dimension along which the entropy is\n",
            "            computed.\n",
            "        ignore_class: Optional integer. The ID of a class to be ignored during\n",
            "            loss computation. This is useful, for example, in segmentation\n",
            "            problems featuring a \"void\" class (commonly -1 or 255) in\n",
            "            segmentation maps. By default (`ignore_class=None`), all classes are\n",
            "            considered.\n",
            "    \n",
            "    Returns:\n",
            "        Sparse categorical crossentropy loss value.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(sparse_categorical_crossentropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4b7f529",
      "metadata": {
        "id": "e4b7f529"
      },
      "source": [
        "Note: from_logits=False is for the situations that we do not have one hot encoded. Since here we have one hot encoded we need to convert it to True for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "ed7d7b4b",
      "metadata": {
        "id": "ed7d7b4b"
      },
      "outputs": [],
      "source": [
        "def spars_cat_loss(y_true,y_pred):\n",
        "    return sparse_categorical_crossentropy(y_true,y_pred,from_logits=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e54c0ff1",
      "metadata": {
        "id": "e54c0ff1"
      },
      "source": [
        "What is an Embedding Layer?\n",
        "\n",
        "An Embedding layer is like a dictionary that translates words (or tokens) into vectors of numbers. These vectors capture the meaning or context of the words in a way that the neural network can understand.\n",
        "\n",
        "Why Use an Embedding Layer?\n",
        "\n",
        "Words are not numbers, but neural networks need numbers to work. Instead of using simple numbers (like 1 for \"cat\", 2 for \"dog\"), we use vectors (arrays of numbers) that can capture more complex relationships between words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "e19546ce",
      "metadata": {
        "id": "e19546ce"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f6b9013",
      "metadata": {
        "id": "1f6b9013"
      },
      "source": [
        "Specifically in this code, we have already converted the code to the numbers, Thus why do we need the embedding again?\n",
        "\n",
        "Why Use an Embedding Layer After Encoding Text?\n",
        "\n",
        "Even though we've already converted words or characters into numbers, those numbers themselves don't capture the complex relationships and meanings of the words. An Embedding layer helps to create meaningful numerical representations (vectors) for each word or character.\n",
        "\n",
        "Simplified Explanation\n",
        "\n",
        "Initial Encoding:\n",
        "\n",
        "We convert characters or words into unique numbers. For example, 'a' might be 1, 'b' might be 2, and so on.\n",
        "This encoding ensures each character or word has a unique identifier but doesn't convey any deeper meaning or relationship.\n",
        "Embedding Layer:\n",
        "\n",
        "Transforms these unique numbers into dense vectors of fixed size.\n",
        "These vectors capture semantic information and relationships between characters or words.\n",
        "\n",
        "Example to Illustrate\n",
        "\n",
        "Initial Encoding:\n",
        "\n",
        "Imagine we have a sentence: \"I love cats.\"\n",
        "\n",
        "Character Encoding:\n",
        "'I' = 1\n",
        "' ' (space) = 2\n",
        "'l' = 3\n",
        "'o' = 4\n",
        "'v' = 5\n",
        "'e' = 6\n",
        "'c' = 7\n",
        "'a' = 8\n",
        "'t' = 9\n",
        "'s' = 10\n",
        "\n",
        "So, \"I love cats.\" is encoded as: [1, 2, 3, 4, 5, 6, 2, 7, 8, 9, 10]\n",
        "\n",
        "Embedding Layer Transformation:\n",
        "\n",
        "Let's say our embedding dimension is 4. Each character will be represented by a 4-dimensional vector. The Embedding layer will transform the numbers into these vectors:\n",
        "\n",
        "'I' (1) -> [0.1, 0.2, 0.3, 0.4]\n",
        "\n",
        "' ' (2) -> [0.2, 0.1, 0.3, 0.5]\n",
        "\n",
        "'l' (3) -> [0.3, 0.3, 0.4, 0.6]\n",
        "\n",
        "'o' (4) -> [0.4, 0.5, 0.1, 0.2]\n",
        "\n",
        "'v' (5) -> [0.5, 0.4, 0.2, 0.3]\n",
        "\n",
        "...\n",
        "These vectors are learned during training to capture meaningful relationships between characters or words.\n",
        "\n",
        "Why This Matters\n",
        "\n",
        "Capturing Meaning:\n",
        "\n",
        "The number '1' for 'I' and '7' for 'c' don't tell us anything about the relationship between 'I' and 'c'.\n",
        "The embedding vectors, however, can show that 'I' is closer in meaning to another pronoun like 'you' than to a noun like 'cat'.\n",
        "Improving Performance:\n",
        "\n",
        "Neural networks can learn patterns and relationships more effectively with these dense vectors than with sparse, high-dimensional one-hot encodings or simple integers.\n",
        "Real-World Example: Word Embeddings\n",
        "In NLP tasks, word embeddings are crucial for understanding context. For instance:\n",
        "\n",
        "\"king\" might be represented as [0.25, 0.65, 0.75, 0.55]\n",
        "\n",
        "\"queen\" might be represented as [0.30, 0.70, 0.80, 0.60]\n",
        "\n",
        "These vectors can show that \"king\" and \"queen\" are related and capture the concept of royalty."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8322c44",
      "metadata": {
        "id": "c8322c44"
      },
      "source": [
        "\n",
        "\n",
        "What is Batch size????\n",
        "\n",
        "\n",
        "\n",
        "In the context of defining a model with Keras (or TensorFlow), the parameter batch_input_shape=[batch_size, None] specifies the shape of the input data that the model expects. Let's break it down:\n",
        "\n",
        "Understanding batch_input_shape=[batch_size, None]\n",
        "\n",
        "Batch Size (batch_size):\n",
        "\n",
        "This represents the number of samples that will be processed together in one forward/backward pass through the network.\n",
        "For example, if batch_size is 32, then the model will process 32 samples at a time.\n",
        "Sequence Length (None):\n",
        "\n",
        "The None indicates that the sequence length is variable. In other words, each batch can contain sequences (e.g., sentences or time series data) of different lengths.\n",
        "This flexibility is particularly useful for working with data where sequences are not of fixed length, such as sentences of varying lengths in NLP.\n",
        "Example: Text Data with Variable Length\n",
        "Consider a batch of sentences being fed into the model. Sentences can have different lengths:\n",
        "\n",
        "Sentence 1: \"I love cats\"\n",
        "Sentence 2: \"Cats are great pets\"\n",
        "Sentence 3: \"Dogs are also great\"\n",
        "When these sentences are tokenized and converted to sequences of numbers (e.g., through an embedding layer), they may have different lengths:\n",
        "\n",
        "Sentence 1: [1, 2, 3]\n",
        "\n",
        "Sentence 2: [3, 4, 5, 6]\n",
        "\n",
        "Sentence 3: [7, 8, 9]\n",
        "\n",
        "Here, None allows for this variability, so the model can process each sentence regardless of its length."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42a93e8e",
      "metadata": {
        "id": "42a93e8e"
      },
      "source": [
        "What is state?\n",
        "\n",
        "In the context of GRUs (Gated Recurrent Units) and other recurrent neural networks (RNNs), the \"state\" refers to the hidden state, which is a representation of the information the network has learned from the sequence so far. This hidden state is essentially the memory of the network, allowing it to retain information across different time steps in the sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4e2f227",
      "metadata": {
        "id": "b4e2f227"
      },
      "source": [
        "Stateful=True:\n",
        "The hidden state is carried over from one batch to the next.\n",
        "This allows the network to retain context even if a long sequence is divided into smaller pieces for processing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b81c2587",
      "metadata": {
        "id": "b81c2587"
      },
      "source": [
        "GRU Layer:\n",
        "\n",
        "A GRU is similar to an LSTM (Long Short-Term Memory) but simpler. It has gates that control the flow of information.\n",
        "It processes sequences one element at a time and maintains a hidden state (memory) that helps it remember previous elements.\n",
        "rnn_neurons:\n",
        "\n",
        "This is the number of units (neurons) in the GRU layer.\n",
        "Think of it as the layer’s memory capacity. More units mean the layer can capture more complex patterns but also requires more computational power.\n",
        "Example: rnn_neurons = 512 means the GRU layer has 512 units.\n",
        "return_sequences=True:\n",
        "\n",
        "This parameter tells the GRU to return the full sequence of outputs for each input sequence, not just the last output.\n",
        "Useful when stacking multiple RNN layers or when each time step’s output is needed for the next layer.\n",
        "Example: If the input sequence has 5 time steps, the GRU will output 5 sequences of vectors.\n",
        "stateful=True:\n",
        "\n",
        "When stateful=True, the GRU maintains the hidden state across batches. This is useful for tasks where the context from previous batches is important, like when processing long sequences split into smaller batches.\n",
        "Example: If processing a long text split into smaller pieces, stateful=True helps the GRU remember the context from one piece to the next.\n",
        "recurrent_initializer='glorot_uniform':\n",
        "\n",
        "This sets the method for initializing the weights of the GRU.\n",
        "glorot_uniform is a common initializer that helps the network start with weights that are neither too large nor too small.\n",
        "It’s like giving the model a good starting point for learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "ecca6671",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecca6671",
        "outputId": "b8c1a07f-a170-42e0-c227-c2968e4faa67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class Embedding in module keras.src.layers.core.embedding:\n",
            "\n",
            "class Embedding(keras.src.engine.base_layer.Layer)\n",
            " |  Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None, sparse=False, **kwargs)\n",
            " |  \n",
            " |  Turns positive integers (indexes) into dense vectors of fixed size.\n",
            " |  \n",
            " |  e.g. `[[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]`\n",
            " |  \n",
            " |  This layer can only be used on positive integer inputs of a fixed range. The\n",
            " |  `tf.keras.layers.TextVectorization`, `tf.keras.layers.StringLookup`,\n",
            " |  and `tf.keras.layers.IntegerLookup` preprocessing layers can help prepare\n",
            " |  inputs for an `Embedding` layer.\n",
            " |  \n",
            " |  This layer accepts `tf.Tensor`, `tf.RaggedTensor` and `tf.SparseTensor`\n",
            " |  input.\n",
            " |  \n",
            " |  Example:\n",
            " |  \n",
            " |  >>> model = tf.keras.Sequential()\n",
            " |  >>> model.add(tf.keras.layers.Embedding(1000, 64, input_length=10))\n",
            " |  >>> # The model will take as input an integer matrix of size (batch,\n",
            " |  >>> # input_length), and the largest integer (i.e. word index) in the input\n",
            " |  >>> # should be no larger than 999 (vocabulary size).\n",
            " |  >>> # Now model.output_shape is (None, 10, 64), where `None` is the batch\n",
            " |  >>> # dimension.\n",
            " |  >>> input_array = np.random.randint(1000, size=(32, 10))\n",
            " |  >>> model.compile('rmsprop', 'mse')\n",
            " |  >>> output_array = model.predict(input_array)\n",
            " |  >>> print(output_array.shape)\n",
            " |  (32, 10, 64)\n",
            " |  \n",
            " |  Args:\n",
            " |    input_dim: Integer. Size of the vocabulary,\n",
            " |      i.e. maximum integer index + 1.\n",
            " |    output_dim: Integer. Dimension of the dense embedding.\n",
            " |    embeddings_initializer: Initializer for the `embeddings`\n",
            " |      matrix (see `keras.initializers`).\n",
            " |    embeddings_regularizer: Regularizer function applied to\n",
            " |      the `embeddings` matrix (see `keras.regularizers`).\n",
            " |    embeddings_constraint: Constraint function applied to\n",
            " |      the `embeddings` matrix (see `keras.constraints`).\n",
            " |    mask_zero: Boolean, whether or not the input value 0 is a special\n",
            " |      \"padding\" value that should be masked out. This is useful when using\n",
            " |      recurrent layers which may take variable length input. If this is\n",
            " |      `True`, then all subsequent layers in the model need to support masking\n",
            " |      or an exception will be raised. If mask_zero is set to True, as a\n",
            " |      consequence, index 0 cannot be used in the vocabulary (input_dim should\n",
            " |      equal size of vocabulary + 1).\n",
            " |    input_length: Length of input sequences, when it is constant.\n",
            " |      This argument is required if you are going to connect\n",
            " |      `Flatten` then `Dense` layers upstream\n",
            " |      (without it, the shape of the dense outputs cannot be computed).\n",
            " |    sparse: If True, calling this layer returns a `tf.SparseTensor`. If False,\n",
            " |      the layer returns a dense `tf.Tensor`. For an entry with no features in\n",
            " |      a sparse tensor (entry with value 0), the embedding vector of index 0 is\n",
            " |      returned by default.\n",
            " |  \n",
            " |  Input shape:\n",
            " |    2D tensor with shape: `(batch_size, input_length)`.\n",
            " |  \n",
            " |  Output shape:\n",
            " |    3D tensor with shape: `(batch_size, input_length, output_dim)`.\n",
            " |  \n",
            " |  **Note on variable placement:**\n",
            " |  By default, if a GPU is available, the embedding matrix will be placed on\n",
            " |  the GPU. This achieves the best performance, but it might cause issues:\n",
            " |  \n",
            " |  - You may be using an optimizer that does not support sparse GPU kernels.\n",
            " |  In this case you will see an error upon training your model.\n",
            " |  - Your embedding matrix may be too large to fit on your GPU. In this case\n",
            " |  you will see an Out Of Memory (OOM) error.\n",
            " |  \n",
            " |  In such cases, you should place the embedding matrix on the CPU memory.\n",
            " |  You can do so with a device scope, as such:\n",
            " |  \n",
            " |  ```python\n",
            " |  with tf.device('cpu:0'):\n",
            " |    embedding_layer = Embedding(...)\n",
            " |    embedding_layer.build()\n",
            " |  ```\n",
            " |  \n",
            " |  The pre-built `embedding_layer` instance can then be added to a `Sequential`\n",
            " |  model (e.g. `model.add(embedding_layer)`), called in a Functional model\n",
            " |  (e.g. `x = embedding_layer(x)`), or used in a subclassed model.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Embedding\n",
            " |      keras.src.engine.base_layer.Layer\n",
            " |      tensorflow.python.module.module.Module\n",
            " |      tensorflow.python.trackable.autotrackable.AutoTrackable\n",
            " |      tensorflow.python.trackable.base.Trackable\n",
            " |      keras.src.utils.version_utils.LayerVersionSelector\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None, sparse=False, **kwargs)\n",
            " |  \n",
            " |  build = wrapper(instance, input_shape)\n",
            " |  \n",
            " |  call(self, inputs)\n",
            " |      This is where the layer's logic lives.\n",
            " |      \n",
            " |      The `call()` method may not create state (except in its first\n",
            " |      invocation, wrapping the creation of variables or other resources in\n",
            " |      `tf.init_scope()`).  It is recommended to create state, including\n",
            " |      `tf.Variable` instances and nested `Layer` instances,\n",
            " |       in `__init__()`, or in the `build()` method that is\n",
            " |      called automatically before `call()` executes for the first time.\n",
            " |      \n",
            " |      Args:\n",
            " |        inputs: Input tensor, or dict/list/tuple of input tensors.\n",
            " |          The first positional `inputs` argument is subject to special rules:\n",
            " |          - `inputs` must be explicitly passed. A layer cannot have zero\n",
            " |            arguments, and `inputs` cannot be provided via the default value\n",
            " |            of a keyword argument.\n",
            " |          - NumPy array or Python scalar values in `inputs` get cast as\n",
            " |            tensors.\n",
            " |          - Keras mask metadata is only collected from `inputs`.\n",
            " |          - Layers are built (`build(input_shape)` method)\n",
            " |            using shape info from `inputs` only.\n",
            " |          - `input_spec` compatibility is only checked against `inputs`.\n",
            " |          - Mixed precision input casting is only applied to `inputs`.\n",
            " |            If a layer has tensor arguments in `*args` or `**kwargs`, their\n",
            " |            casting behavior in mixed precision should be handled manually.\n",
            " |          - The SavedModel input specification is generated using `inputs`\n",
            " |            only.\n",
            " |          - Integration with various ecosystem packages like TFMOT, TFLite,\n",
            " |            TF.js, etc is only supported for `inputs` and not for tensors in\n",
            " |            positional and keyword arguments.\n",
            " |        *args: Additional positional arguments. May contain tensors, although\n",
            " |          this is not recommended, for the reasons above.\n",
            " |        **kwargs: Additional keyword arguments. May contain tensors, although\n",
            " |          this is not recommended, for the reasons above.\n",
            " |          The following optional keyword arguments are reserved:\n",
            " |          - `training`: Boolean scalar tensor of Python boolean indicating\n",
            " |            whether the `call` is meant for training or inference.\n",
            " |          - `mask`: Boolean input mask. If the layer's `call()` method takes a\n",
            " |            `mask` argument, its default value will be set to the mask\n",
            " |            generated for `inputs` by the previous layer (if `input` did come\n",
            " |            from a layer that generated a corresponding mask, i.e. if it came\n",
            " |            from a Keras layer with masking support).\n",
            " |      \n",
            " |      Returns:\n",
            " |        A tensor or list/tuple of tensors.\n",
            " |  \n",
            " |  compute_mask(self, inputs, mask=None)\n",
            " |      Computes an output mask tensor.\n",
            " |      \n",
            " |      Args:\n",
            " |          inputs: Tensor or list of tensors.\n",
            " |          mask: Tensor or list of tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |          None or a tensor (or list of tensors,\n",
            " |              one per output tensor of the layer).\n",
            " |  \n",
            " |  compute_output_shape = wrapper(instance, input_shape)\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the config of the layer.\n",
            " |      \n",
            " |      A layer config is a Python dictionary (serializable)\n",
            " |      containing the configuration of a layer.\n",
            " |      The same layer can be reinstantiated later\n",
            " |      (without its trained weights) from this configuration.\n",
            " |      \n",
            " |      The config of a layer does not include connectivity\n",
            " |      information, nor the layer class name. These are handled\n",
            " |      by `Network` (one layer of abstraction above).\n",
            " |      \n",
            " |      Note that `get_config()` does not guarantee to return a fresh copy of\n",
            " |      dict every time it is called. The callers should make a copy of the\n",
            " |      returned dict if they want to modify it.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Python dictionary.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.src.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __call__(self, *args, **kwargs)\n",
            " |      Wraps `call`, applying pre- and post-processing steps.\n",
            " |      \n",
            " |      Args:\n",
            " |        *args: Positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |      \n",
            " |      Note:\n",
            " |        - The following optional keyword arguments are reserved for specific\n",
            " |          uses:\n",
            " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
            " |            whether the `call` is meant for training or inference.\n",
            " |          * `mask`: Boolean input mask.\n",
            " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
            " |          layers do), its default value will be set to the mask generated\n",
            " |          for `inputs` by the previous layer (if `input` did come from\n",
            " |          a layer that generated a corresponding mask, i.e. if it came from\n",
            " |          a Keras layer with masking support.\n",
            " |        - If the layer is not built, the method will call `build`.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: if the layer's `call` method returns None (an invalid\n",
            " |          value).\n",
            " |        RuntimeError: if `super().__init__()` was not called in the\n",
            " |          constructor.\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Support self.foo = trackable syntax.\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  add_loss(self, losses, **kwargs)\n",
            " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Some losses (for instance, activity regularization losses) may be\n",
            " |      dependent on the inputs passed when calling a layer. Hence, when reusing\n",
            " |      the same layer on different inputs `a` and `b`, some entries in\n",
            " |      `layer.losses` may be dependent on `a` and some on `b`. This method\n",
            " |      automatically keeps track of dependencies.\n",
            " |      \n",
            " |      This method can be used inside a subclassed layer or model's `call`\n",
            " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyLayer(tf.keras.layers.Layer):\n",
            " |        def call(self, inputs):\n",
            " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      The same code works in distributed training: the input to `add_loss()`\n",
            " |      is treated like a regularization loss and averaged across replicas\n",
            " |      by the training loop (both built-in `Model.fit()` and compliant custom\n",
            " |      training loops).\n",
            " |      \n",
            " |      The `add_loss` method can also be called directly on a Functional Model\n",
            " |      during construction. In this case, any loss Tensors passed to this Model\n",
            " |      must be symbolic and be able to be traced back to the model's `Input`s.\n",
            " |      These losses become part of the model's topology and are tracked in\n",
            " |      `get_config`.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Activity regularization.\n",
            " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      ```\n",
            " |      \n",
            " |      If this is not the case for your loss (if, for example, your loss\n",
            " |      references a `Variable` of one of the model's layers), you can wrap your\n",
            " |      loss in a zero-argument lambda. These losses are not tracked as part of\n",
            " |      the model's topology since they can't be serialized.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      d = tf.keras.layers.Dense(10)\n",
            " |      x = d(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Weight regularization.\n",
            " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors,\n",
            " |          losses may also be zero-argument callables which create a loss\n",
            " |          tensor.\n",
            " |        **kwargs: Used for backwards compatibility only.\n",
            " |  \n",
            " |  add_metric(self, value, name=None, **kwargs)\n",
            " |      Adds metric tensor to the layer.\n",
            " |      \n",
            " |      This method can be used inside the `call()` method of a subclassed layer\n",
            " |      or model.\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
            " |        def __init__(self):\n",
            " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
            " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
            " |      \n",
            " |        def call(self, inputs):\n",
            " |          self.add_metric(self.mean(inputs))\n",
            " |          self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any tensor passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      metrics become part of the model's topology and are tracked when you\n",
            " |      save the model via `save()`.\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
            " |      ```\n",
            " |      \n",
            " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
            " |      Functional Model, as shown in the example below, is not supported. This\n",
            " |      is because we cannot trace the metric result tensor back to the model's\n",
            " |      inputs.\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        value: Metric tensor.\n",
            " |        name: String metric name.\n",
            " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
            " |          Accepted values:\n",
            " |          `aggregation` - When the `value` tensor provided is not the result\n",
            " |          of calling a `keras.Metric` instance, it will be aggregated by\n",
            " |          default using a `keras.Metric.Mean`.\n",
            " |  \n",
            " |  add_update(self, updates)\n",
            " |      Add update op(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Weight updates (for instance, the updates of the moving mean and\n",
            " |      variance in a BatchNormalization layer) may be dependent on the inputs\n",
            " |      passed when calling a layer. Hence, when reusing the same layer on\n",
            " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
            " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This call is ignored when eager execution is enabled (in that case,\n",
            " |      variable updates are run on the fly and thus do not need to be tracked\n",
            " |      for later execution).\n",
            " |      \n",
            " |      Args:\n",
            " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
            " |          that returns an update op. A zero-arg callable should be passed in\n",
            " |          order to disable running the updates by setting `trainable=False`\n",
            " |          on this Layer, when executing in Eager mode.\n",
            " |  \n",
            " |  add_variable(self, *args, **kwargs)\n",
            " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
            " |  \n",
            " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregationV2.NONE: 0>, **kwargs)\n",
            " |      Adds a new variable to the layer.\n",
            " |      \n",
            " |      Args:\n",
            " |        name: Variable name.\n",
            " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
            " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
            " |        initializer: Initializer instance (callable).\n",
            " |        regularizer: Regularizer instance (callable).\n",
            " |        trainable: Boolean, whether the variable should be part of the layer's\n",
            " |          \"trainable_variables\" (e.g. variables, biases)\n",
            " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
            " |          Note that `trainable` cannot be `True` if `synchronization`\n",
            " |          is set to `ON_READ`.\n",
            " |        constraint: Constraint instance (callable).\n",
            " |        use_resource: Whether to use a `ResourceVariable` or not.\n",
            " |          See [this guide](\n",
            " |          https://www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables)\n",
            " |           for more information.\n",
            " |        synchronization: Indicates when a distributed a variable will be\n",
            " |          aggregated. Accepted values are constants defined in the class\n",
            " |          `tf.VariableSynchronization`. By default the synchronization is set\n",
            " |          to `AUTO` and the current `DistributionStrategy` chooses when to\n",
            " |          synchronize. If `synchronization` is set to `ON_READ`, `trainable`\n",
            " |          must not be set to `True`.\n",
            " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
            " |          Accepted values are constants defined in the class\n",
            " |          `tf.VariableAggregation`.\n",
            " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
            " |          `collections`, `experimental_autocast` and `caching_device`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The variable created.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: When giving unsupported dtype and no initializer or when\n",
            " |          trainable has been set to True with synchronization set as\n",
            " |          `ON_READ`.\n",
            " |  \n",
            " |  build_from_config(self, config)\n",
            " |      Builds the layer's states with the supplied config dict.\n",
            " |      \n",
            " |      By default, this method calls the `build(config[\"input_shape\"])` method,\n",
            " |      which creates weights based on the layer's input shape in the supplied\n",
            " |      config. If your config contains other information needed to load the\n",
            " |      layer's state, you should override this method.\n",
            " |      \n",
            " |      Args:\n",
            " |          config: Dict containing the input shape associated with this layer.\n",
            " |  \n",
            " |  compute_output_signature(self, input_signature)\n",
            " |      Compute the output tensor signature of the layer based on the inputs.\n",
            " |      \n",
            " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
            " |      and dtype information for a tensor. This method allows layers to provide\n",
            " |      output dtype information if it is different from the input dtype.\n",
            " |      For any layer that doesn't implement this function,\n",
            " |      the framework will fall back to use `compute_output_shape`, and will\n",
            " |      assume that the output dtype matches the input dtype.\n",
            " |      \n",
            " |      Args:\n",
            " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
            " |          objects, describing a candidate input for the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Single TensorSpec or nested structure of TensorSpec objects,\n",
            " |          describing how the layer would transform the provided input.\n",
            " |      \n",
            " |      Raises:\n",
            " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
            " |  \n",
            " |  count_params(self)\n",
            " |      Count the total number of scalars composing the weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An integer count.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: if the layer isn't yet built\n",
            " |            (in which case its weights aren't yet defined).\n",
            " |  \n",
            " |  finalize_state(self)\n",
            " |      Finalizes the layers state after updating layer weights.\n",
            " |      \n",
            " |      This function can be subclassed in a layer and will be called after\n",
            " |      updating a layer weights. It can be overridden to finalize any\n",
            " |      additional layer state after a weight update.\n",
            " |      \n",
            " |      This function will be called after weights of a layer have been restored\n",
            " |      from a loaded model.\n",
            " |  \n",
            " |  get_build_config(self)\n",
            " |      Returns a dictionary with the layer's input shape.\n",
            " |      \n",
            " |      This method returns a config dict that can be used by\n",
            " |      `build_from_config(config)` to create all states (e.g. Variables and\n",
            " |      Lookup tables) needed by the layer.\n",
            " |      \n",
            " |      By default, the config only contains the input shape that the layer\n",
            " |      was built with. If you're writing a custom layer that creates state in\n",
            " |      an unusual way, you should override this method to make sure this state\n",
            " |      is already created when Keras attempts to load its value upon model\n",
            " |      loading.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A dict containing the input shape associated with the layer.\n",
            " |  \n",
            " |  get_input_at(self, node_index)\n",
            " |      Retrieves the input tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first input node of the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_input_mask_at(self, node_index)\n",
            " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_shape_at(self, node_index)\n",
            " |      Retrieves the input shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_output_at(self, node_index)\n",
            " |      Retrieves the output tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first output node of the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_output_mask_at(self, node_index)\n",
            " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_shape_at(self, node_index)\n",
            " |      Retrieves the output shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_weights(self)\n",
            " |      Returns the current weights of the layer, as NumPy arrays.\n",
            " |      \n",
            " |      The weights of a layer represent the state of the layer. This function\n",
            " |      returns both trainable and non-trainable weight values associated with\n",
            " |      this layer as a list of NumPy arrays, which can in turn be used to load\n",
            " |      state into similarly parameterized layers.\n",
            " |      \n",
            " |      For example, a `Dense` layer returns a list of two values: the kernel\n",
            " |      matrix and the bias vector. These can be used to set the weights of\n",
            " |      another `Dense` layer:\n",
            " |      \n",
            " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
            " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
            " |      >>> layer_a.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
            " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
            " |      >>> layer_b.get_weights()\n",
            " |      [array([[2.],\n",
            " |             [2.],\n",
            " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
            " |      >>> layer_b.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      \n",
            " |      Returns:\n",
            " |          Weights values as a list of NumPy arrays.\n",
            " |  \n",
            " |  load_own_variables(self, store)\n",
            " |      Loads the state of the layer.\n",
            " |      \n",
            " |      You can override this method to take full control of how the state of\n",
            " |      the layer is loaded upon calling `keras.models.load_model()`.\n",
            " |      \n",
            " |      Args:\n",
            " |          store: Dict from which the state of the model will be loaded.\n",
            " |  \n",
            " |  save_own_variables(self, store)\n",
            " |      Saves the state of the layer.\n",
            " |      \n",
            " |      You can override this method to take full control of how the state of\n",
            " |      the layer is saved upon calling `model.save()`.\n",
            " |      \n",
            " |      Args:\n",
            " |          store: Dict where the state of the model will be saved.\n",
            " |  \n",
            " |  set_weights(self, weights)\n",
            " |      Sets the weights of the layer, from NumPy arrays.\n",
            " |      \n",
            " |      The weights of a layer represent the state of the layer. This function\n",
            " |      sets the weight values from numpy arrays. The weight values should be\n",
            " |      passed in the order they are created by the layer. Note that the layer's\n",
            " |      weights must be instantiated before calling this function, by calling\n",
            " |      the layer.\n",
            " |      \n",
            " |      For example, a `Dense` layer returns a list of two values: the kernel\n",
            " |      matrix and the bias vector. These can be used to set the weights of\n",
            " |      another `Dense` layer:\n",
            " |      \n",
            " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
            " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
            " |      >>> layer_a.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
            " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
            " |      >>> layer_b.get_weights()\n",
            " |      [array([[2.],\n",
            " |             [2.],\n",
            " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
            " |      >>> layer_b.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      \n",
            " |      Args:\n",
            " |        weights: a list of NumPy arrays. The number\n",
            " |          of arrays and their shape must match\n",
            " |          number of the dimensions of the weights\n",
            " |          of the layer (i.e. it should match the\n",
            " |          output of `get_weights`).\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: If the provided weights list does not match the\n",
            " |          layer's specifications.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from keras.src.engine.base_layer.Layer:\n",
            " |  \n",
            " |  from_config(config) from builtins.type\n",
            " |      Creates a layer from its config.\n",
            " |      \n",
            " |      This method is the reverse of `get_config`,\n",
            " |      capable of instantiating the same layer from the config\n",
            " |      dictionary. It does not handle layer connectivity\n",
            " |      (handled by Network), nor weights (handled by `set_weights`).\n",
            " |      \n",
            " |      Args:\n",
            " |          config: A Python dictionary, typically the\n",
            " |              output of get_config.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from keras.src.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __new__(cls, *args, **kwargs)\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from keras.src.engine.base_layer.Layer:\n",
            " |  \n",
            " |  compute_dtype\n",
            " |      The dtype of the layer's computations.\n",
            " |      \n",
            " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
            " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
            " |      the weights.\n",
            " |      \n",
            " |      Layers automatically cast their inputs to the compute dtype, which\n",
            " |      causes computations and the output to be in the compute dtype as well.\n",
            " |      This is done by the base Layer class in `Layer.__call__`, so you do not\n",
            " |      have to insert these casts if implementing your own layer.\n",
            " |      \n",
            " |      Layers often perform certain internal computations in higher precision\n",
            " |      when `compute_dtype` is float16 or bfloat16 for numeric stability. The\n",
            " |      output will still typically be float16 or bfloat16 in such cases.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The layer's compute dtype.\n",
            " |  \n",
            " |  dtype\n",
            " |      The dtype of the layer weights.\n",
            " |      \n",
            " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
            " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
            " |      dtype of the layer's computations.\n",
            " |  \n",
            " |  dtype_policy\n",
            " |      The dtype policy associated with this layer.\n",
            " |      \n",
            " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
            " |  \n",
            " |  dynamic\n",
            " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
            " |  \n",
            " |  inbound_nodes\n",
            " |      Return Functional API nodes upstream of this layer.\n",
            " |  \n",
            " |  input\n",
            " |      Retrieves the input tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input tensor or list of input tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |        AttributeError: If no inbound nodes are found.\n",
            " |  \n",
            " |  input_mask\n",
            " |      Retrieves the input mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input mask tensor (potentially None) or list of input\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_shape\n",
            " |      Retrieves the input shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
            " |      have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per input tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined input_shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  losses\n",
            " |      List of losses added using the `add_loss()` API.\n",
            " |      \n",
            " |      Variable regularization tensors are created when this property is\n",
            " |      accessed, so it is eager safe: accessing `losses` under a\n",
            " |      `tf.GradientTape` will propagate gradients back to the corresponding\n",
            " |      variables.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
            " |      ...   def call(self, inputs):\n",
            " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
            " |      ...     return inputs\n",
            " |      >>> l = MyLayer()\n",
            " |      >>> l(np.ones((10, 1)))\n",
            " |      >>> l.losses\n",
            " |      [1.0]\n",
            " |      \n",
            " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
            " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      >>> model = tf.keras.Model(inputs, outputs)\n",
            " |      >>> # Activity regularization.\n",
            " |      >>> len(model.losses)\n",
            " |      0\n",
            " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      >>> len(model.losses)\n",
            " |      1\n",
            " |      \n",
            " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
            " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
            " |      >>> x = d(inputs)\n",
            " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      >>> model = tf.keras.Model(inputs, outputs)\n",
            " |      >>> # Weight regularization.\n",
            " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
            " |      >>> model.losses\n",
            " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of tensors.\n",
            " |  \n",
            " |  metrics\n",
            " |      List of metrics attached to the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A list of `Metric` objects.\n",
            " |  \n",
            " |  name\n",
            " |      Name of the layer (string), set in the constructor.\n",
            " |  \n",
            " |  non_trainable_variables\n",
            " |      Sequence of non-trainable variables owned by this module and its submodules.\n",
            " |      \n",
            " |      Note: this method uses reflection to find variables on the current instance\n",
            " |      and submodules. For performance reasons you may wish to cache the result\n",
            " |      of calling this method if you don't expect the return value to change.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of variables for the current module (sorted by attribute\n",
            " |        name) followed by variables from all submodules recursively (breadth\n",
            " |        first).\n",
            " |  \n",
            " |  non_trainable_weights\n",
            " |      List of all non-trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Non-trainable weights are *not* updated during training. They are\n",
            " |      expected to be updated manually in `call()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of non-trainable variables.\n",
            " |  \n",
            " |  outbound_nodes\n",
            " |      Return Functional API nodes downstream of this layer.\n",
            " |  \n",
            " |  output\n",
            " |      Retrieves the output tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one output,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor or list of output tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        AttributeError: if the layer is connected to more than one incoming\n",
            " |          layers.\n",
            " |        RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  output_mask\n",
            " |      Retrieves the output mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output mask tensor (potentially None) or list of output\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_shape\n",
            " |      Retrieves the output shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has one output,\n",
            " |      or if all outputs have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per output tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined output shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  trainable_variables\n",
            " |      Sequence of trainable variables owned by this module and its submodules.\n",
            " |      \n",
            " |      Note: this method uses reflection to find variables on the current instance\n",
            " |      and submodules. For performance reasons you may wish to cache the result\n",
            " |      of calling this method if you don't expect the return value to change.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of variables for the current module (sorted by attribute\n",
            " |        name) followed by variables from all submodules recursively (breadth\n",
            " |        first).\n",
            " |  \n",
            " |  trainable_weights\n",
            " |      List of all trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Trainable weights are updated via gradient descent during training.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of trainable variables.\n",
            " |  \n",
            " |  updates\n",
            " |  \n",
            " |  variable_dtype\n",
            " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
            " |  \n",
            " |  variables\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Alias of `self.weights`.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are\n",
            " |      not themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  weights\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from keras.src.engine.base_layer.Layer:\n",
            " |  \n",
            " |  activity_regularizer\n",
            " |      Optional regularizer function for the output of this layer.\n",
            " |  \n",
            " |  input_spec\n",
            " |      `InputSpec` instance(s) describing the input format for this layer.\n",
            " |      \n",
            " |      When you create a layer subclass, you can set `self.input_spec` to\n",
            " |      enable the layer to run input compatibility checks when it is called.\n",
            " |      Consider a `Conv2D` layer: it can only be called on a single input\n",
            " |      tensor of rank 4. As such, you can set, in `__init__()`:\n",
            " |      \n",
            " |      ```python\n",
            " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
            " |      ```\n",
            " |      \n",
            " |      Now, if you try to call the layer on an input that isn't rank 4\n",
            " |      (for instance, an input of shape `(2,)`, it will raise a\n",
            " |      nicely-formatted error:\n",
            " |      \n",
            " |      ```\n",
            " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
            " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
            " |      ```\n",
            " |      \n",
            " |      Input checks that can be specified via `input_spec` include:\n",
            " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
            " |      - Shape\n",
            " |      - Rank (ndim)\n",
            " |      - Dtype\n",
            " |      \n",
            " |      For more information, see `tf.keras.layers.InputSpec`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
            " |  \n",
            " |  stateful\n",
            " |  \n",
            " |  supports_masking\n",
            " |      Whether this layer supports computing a mask using `compute_mask`.\n",
            " |  \n",
            " |  trainable\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  with_name_scope(method) from builtins.type\n",
            " |      Decorator to automatically enter the module name scope.\n",
            " |      \n",
            " |      >>> class MyModule(tf.Module):\n",
            " |      ...   @tf.Module.with_name_scope\n",
            " |      ...   def __call__(self, x):\n",
            " |      ...     if not hasattr(self, 'w'):\n",
            " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
            " |      ...     return tf.matmul(x, self.w)\n",
            " |      \n",
            " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
            " |      names included the module name:\n",
            " |      \n",
            " |      >>> mod = MyModule()\n",
            " |      >>> mod(tf.ones([1, 2]))\n",
            " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
            " |      >>> mod.w\n",
            " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
            " |      numpy=..., dtype=float32)>\n",
            " |      \n",
            " |      Args:\n",
            " |        method: The method to wrap.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The original method wrapped such that it enters the module's name scope.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  name_scope\n",
            " |      Returns a `tf.name_scope` instance for this class.\n",
            " |  \n",
            " |  submodules\n",
            " |      Sequence of all sub-modules.\n",
            " |      \n",
            " |      Submodules are modules which are properties of this module, or found as\n",
            " |      properties of modules which are properties of this module (and so on).\n",
            " |      \n",
            " |      >>> a = tf.Module()\n",
            " |      >>> b = tf.Module()\n",
            " |      >>> c = tf.Module()\n",
            " |      >>> a.b = b\n",
            " |      >>> b.c = c\n",
            " |      >>> list(a.submodules) == [b, c]\n",
            " |      True\n",
            " |      >>> list(b.submodules) == [c]\n",
            " |      True\n",
            " |      >>> list(c.submodules) == []\n",
            " |      True\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of all submodules.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.trackable.base.Trackable:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(Embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "122df192",
      "metadata": {
        "id": "122df192"
      },
      "outputs": [],
      "source": [
        "#help(GRU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "c923b953",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c923b953",
        "outputId": "ddc56e6a-fb6a-4848-b3b0-66b1466a1831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "0509a650",
      "metadata": {
        "id": "0509a650"
      },
      "outputs": [],
      "source": [
        "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
        "    model=Sequential()\n",
        "    model.add(Embedding(vocab_size, embed_dim,batch_input_shape=[batch_size, None]))\n",
        "    model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n",
        "    model.add(Dense(vocab_size))\n",
        "    model.compile('adam', loss=spars_cat_loss)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "de3efa64",
      "metadata": {
        "id": "de3efa64"
      },
      "outputs": [],
      "source": [
        "model = create_model(\n",
        "  vocab_size = vocab_size,\n",
        "  embed_dim=embedding_dim,\n",
        "  rnn_neurons=rnn_neurons,\n",
        "  batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "87e943f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87e943f9",
        "outputId": "d977e513-1f36-4ec8-f0e2-3236adb4b9dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (128, None, 64)           5376      \n",
            "                                                                 \n",
            " gru (GRU)                   (128, None, 1026)         3361176   \n",
            "                                                                 \n",
            " dense (Dense)               (128, None, 84)           86268     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3452820 (13.17 MB)\n",
            "Trainable params: 3452820 (13.17 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "\n",
        "  # Predict off some random batch\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "\n",
        "  # Display the dimensions of the predictions\n",
        "  print(example_batch_predictions.shape, \" <=== (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGAqowfjkO4B",
        "outputId": "c5326ec0-e281-4494-e2a9-dd6f6013e976"
      },
      "id": "oGAqowfjkO4B",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 120, 84)  <=== (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nmRRbMrkRlv",
        "outputId": "e21e6fe0-04b9-46fb-912a-b42c9ca7b047"
      },
      "id": "3nmRRbMrkRlv",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(128, 120, 84), dtype=float32, numpy=\n",
              "array([[[ 6.5142051e-03,  4.2717289e-03, -3.0525592e-03, ...,\n",
              "         -2.8919958e-04, -1.7008002e-04,  4.5880470e-03],\n",
              "        [ 1.0318146e-02,  5.6120940e-03, -5.8209044e-03, ...,\n",
              "         -9.6168910e-04, -1.0971507e-03,  7.7435113e-03],\n",
              "        [ 1.1868290e-02,  3.3632182e-03, -3.1446922e-03, ...,\n",
              "         -6.0167350e-03, -1.3955489e-03,  7.9836659e-03],\n",
              "        ...,\n",
              "        [-1.1457721e-03, -5.1885168e-04, -2.1634847e-03, ...,\n",
              "         -1.3487912e-03, -5.3533246e-03, -2.4909517e-03],\n",
              "        [-6.2023667e-03, -2.2846139e-03, -1.1185110e-03, ...,\n",
              "         -5.1654028e-03, -9.3204877e-04, -1.0762501e-03],\n",
              "        [-8.7586772e-03, -2.5908502e-03, -3.8924394e-04, ...,\n",
              "         -7.0146769e-03,  1.6596145e-03,  1.9030952e-03]],\n",
              "\n",
              "       [[ 6.1810431e-03,  1.8699289e-03,  1.8036326e-03, ...,\n",
              "         -4.6035349e-03,  3.8430496e-04,  2.8282627e-03],\n",
              "        [ 4.5965994e-03, -2.3203460e-04, -3.1072686e-03, ...,\n",
              "         -2.8282774e-03,  2.8682863e-03,  3.8329521e-03],\n",
              "        [ 5.7294541e-03, -2.1119521e-03,  1.4517123e-03, ...,\n",
              "         -4.7952766e-03,  1.5290723e-03, -2.6415852e-03],\n",
              "        ...,\n",
              "        [ 6.1238422e-03, -2.4839644e-03, -6.3565397e-03, ...,\n",
              "         -8.6317677e-04, -2.7102063e-04,  2.0134402e-03],\n",
              "        [ 6.6406518e-04, -5.3519773e-04, -3.4433340e-03, ...,\n",
              "         -8.1584050e-04, -1.2595494e-03,  7.1891554e-06],\n",
              "        [ 2.0851044e-03, -2.6716306e-03, -5.2325013e-03, ...,\n",
              "         -1.7177806e-03,  6.7308359e-04, -1.0112365e-03]],\n",
              "\n",
              "       [[ 6.2690363e-03,  2.3378015e-03,  6.3477480e-04, ...,\n",
              "         -1.3402568e-03, -3.7471802e-04,  2.2375570e-03],\n",
              "        [ 6.4235115e-03,  3.9174897e-03,  2.4480486e-04, ...,\n",
              "         -6.0528670e-03,  9.0425247e-03, -1.3463183e-03],\n",
              "        [ 2.7974255e-03,  2.7023945e-03,  2.8500529e-03, ...,\n",
              "         -5.8035734e-03, -3.7285106e-03, -3.5045750e-03],\n",
              "        ...,\n",
              "        [ 7.9444190e-03, -3.0171708e-03, -2.1234285e-03, ...,\n",
              "         -6.8402640e-03,  1.4874436e-03, -3.1981540e-03],\n",
              "        [ 1.0917609e-02,  3.1973622e-03, -4.2542946e-03, ...,\n",
              "         -4.2124521e-03,  1.4157085e-03,  4.0274044e-03],\n",
              "        [ 8.0534937e-03, -3.4583029e-03, -2.1055376e-03, ...,\n",
              "         -5.8150184e-03, -1.9191108e-03,  3.5055061e-03]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-3.2344132e-03,  1.9002229e-03,  8.1028855e-05, ...,\n",
              "         -9.9887443e-04, -1.9821654e-04, -1.9334528e-03],\n",
              "        [ 4.9966369e-03,  5.5905059e-03, -3.0512461e-03, ...,\n",
              "         -7.9177896e-04,  2.5572759e-04,  3.4167385e-03],\n",
              "        [ 4.6156058e-03,  1.9852296e-03, -1.8433878e-03, ...,\n",
              "         -5.7043316e-04,  1.3460961e-04,  2.6782528e-03],\n",
              "        ...,\n",
              "        [ 1.3108851e-03,  2.5034475e-03, -4.4963406e-03, ...,\n",
              "         -3.4334173e-03, -1.1450439e-03,  5.5369670e-03],\n",
              "        [ 7.5373952e-03,  4.6295277e-03, -6.7284601e-03, ...,\n",
              "         -2.1454196e-03, -1.8565839e-03,  9.0632457e-03],\n",
              "        [ 7.3329923e-03,  6.4192002e-04, -1.2495150e-02, ...,\n",
              "         -5.5005326e-04,  2.4644805e-03,  5.0903461e-03]],\n",
              "\n",
              "       [[ 2.8824114e-04,  2.2166690e-03, -4.3547442e-03, ...,\n",
              "          5.0771842e-04, -4.3346998e-03, -8.8107190e-04],\n",
              "        [-8.3859758e-03,  1.1379309e-04, -5.6988769e-03, ...,\n",
              "          4.7585811e-03, -5.6729405e-03, -5.2797557e-03],\n",
              "        [ 2.1512660e-03,  1.6101471e-03, -2.3974352e-04, ...,\n",
              "         -2.1726722e-03, -1.4647284e-03, -4.0002563e-04],\n",
              "        ...,\n",
              "        [-1.5871760e-03, -2.7102190e-03, -4.1067554e-03, ...,\n",
              "         -5.6950389e-03, -6.7580573e-04,  3.6378819e-03],\n",
              "        [-3.9015396e-04, -1.1460256e-02, -4.9753445e-03, ...,\n",
              "         -4.0503158e-03, -1.5061295e-03,  7.3187072e-03],\n",
              "        [-1.0601397e-03, -9.6769514e-04, -2.2227333e-03, ...,\n",
              "         -7.9417956e-04,  6.6575990e-03, -3.7232754e-03]],\n",
              "\n",
              "       [[ 6.5142089e-03,  4.2717285e-03, -3.0525578e-03, ...,\n",
              "         -2.8919935e-04, -1.7007979e-04,  4.5880489e-03],\n",
              "        [ 3.7715042e-03,  3.3558314e-04, -2.5947152e-03, ...,\n",
              "          2.7358013e-03,  8.4288459e-04,  9.4956737e-03],\n",
              "        [ 1.8938083e-03, -3.0127214e-03, -3.4922382e-03, ...,\n",
              "         -9.5114205e-04, -1.9583127e-03,  5.4483237e-03],\n",
              "        ...,\n",
              "        [ 4.0974636e-03,  1.5808077e-03, -1.1318466e-03, ...,\n",
              "         -4.2240443e-03, -8.4605655e-03,  1.6469997e-04],\n",
              "        [ 5.6883753e-03, -2.8668370e-03, -6.3794185e-03, ...,\n",
              "         -2.0971007e-03,  2.6073714e-04,  1.1959739e-03],\n",
              "        [ 4.1845787e-04, -8.6517655e-04, -3.6580565e-03, ...,\n",
              "         -1.6039165e-03, -4.1185273e-04, -4.7257519e-04]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)"
      ],
      "metadata": {
        "id": "Xy0yMBk0kUvO"
      },
      "id": "Xy0yMBk0kUvO",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "metadata": {
        "id": "EY9aQLUfkYVD"
      },
      "id": "EY9aQLUfkYVD",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Given the input seq: \\n\")\n",
        "print(\"\".join(index_to_char[input_example_batch[0]]))\n",
        "print('\\n')\n",
        "print(\"Next Char Predictions: \\n\")\n",
        "print(\"\".join(index_to_char[sampled_indices ]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMULYvP9ka8T",
        "outputId": "1c26fb4c-bcce-4bea-d1e1-ba33f2c6b0a8"
      },
      "id": "bMULYvP9ka8T",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given the input seq: \n",
            "\n",
            "  The manner of my pity-wanting pain.\n",
            "  If I might teach thee wit better it were,\n",
            "  Though not to love, yet love to tell\n",
            "\n",
            "\n",
            "Next Char Predictions: \n",
            "\n",
            "Co_P_r7b>qXl4(hm136! 1P1[oorA_exd}>EEchs;Lc?M:a\n",
            "d:eh\"YME:-GHqbXUH&<6l8HTlBAD(d:v8X5Fy1:zoWc_<FT):66Zrd.1,soB:btXj9!IROTU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model.fit(dataset,epochs=epochs)"
      ],
      "metadata": {
        "id": "k76evvO5khSe"
      },
      "id": "k76evvO5khSe",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Since the run time is extermely long We would and this code is for leanring we would use the prepared model here!"
      ],
      "metadata": {
        "id": "lzu8146lp34l"
      },
      "id": "lzu8146lp34l"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "103f52f6",
      "metadata": {
        "id": "103f52f6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Instead of loading the entire model we have loaded the weights"
      ],
      "metadata": {
        "id": "TdJW1LdR8b6E"
      },
      "id": "TdJW1LdR8b6E"
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(\n",
        "  vocab_size = vocab_size,\n",
        "  embed_dim=embedding_dim,\n",
        "  rnn_neurons=rnn_neurons,\n",
        "  batch_size=1)\n",
        "\n",
        "#we just upload the weights of the model here\n",
        "\n",
        "model.load_weights(\"/content/shakespeare_gen.h5\")\n",
        "\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "metadata": {
        "id": "YZ_FA7gkqWnV"
      },
      "id": "YZ_FA7gkqWnV",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is model.build(tf.TensorShape([1, None]))?\n",
        "When you create a model in TensorFlow, you need to tell the model what shape of data it will be working with. This line of code does exactly that.\n",
        "\n",
        "Let's Simplify:\n",
        "The Model Needs to Know the Input Shape:\n",
        "\n",
        "Think of the model like a machine that processes data. Before it can start, it needs to know what kind of data it will be processing.\n",
        "Input Shape in Simple Terms:\n",
        "\n",
        "tf.TensorShape([1, None]) is like telling the model, \"Hey, you will get a batch of data where each piece of data can be of any length.\"\n",
        "Here, 1 means each batch will contain just one piece of data.\n",
        "None means the length of each piece of data can be different each time."
      ],
      "metadata": {
        "id": "1bHM7RjZ9T9g"
      },
      "id": "1bHM7RjZ9T9g"
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "5uP0Ae888lV-",
        "outputId": "ed9d1bfe-fa65-4e84-b7e4-c1dd740b416f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5uP0Ae888lV-",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (1, None, 64)             5376      \n",
            "                                                                 \n",
            " gru_2 (GRU)                 (1, None, 1026)           3361176   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (1, None, 84)             86268     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3452820 (13.17 MB)\n",
            "Trainable params: 3452820 (13.17 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_seed, gen_size=500, temp=1):\n",
        "  num_generate= gen_size\n",
        "  #converting Char to index\n",
        "  input_eval=[char_to_index[s] for s in start_seed]\n",
        "  input_eval=tf.expand_dims(input_eval,0)\n",
        "  text_generated=[]\n",
        "  temperature=temp\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions=model(input_eval)\n",
        "    predictions=tf.squeeze(predictions,0)\n",
        "    predictions=predictions/temperature\n",
        "    predicted_id=tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "    input_eval=tf.expand_dims([predicted_id],0)\n",
        "    text_generated.append(index_to_char[predicted_id])\n",
        "\n",
        "  return (start_seed+\"\".join(text_generated))"
      ],
      "metadata": {
        "id": "hc20u6t_9X2w"
      },
      "id": "hc20u6t_9X2w",
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Gen_size is how much character you want to generate!\n",
        "\n",
        "What is input_eval=tf.expand_dims(input_eval, 0)?\n",
        "This line of code takes a sequence of data (in this case, a list of character indices) and adds an extra dimension to it. This is useful for preparing data to be fed into a model.\n",
        "\n",
        "Let's Simplify:\n",
        "Original Data Shape:\n",
        "\n",
        "input_eval is a list of numbers representing characters.\n",
        "Example: [2, 5, 7, 1] (a sequence of character indices).\n",
        "Adding a Dimension:\n",
        "\n",
        "tf.expand_dims(input_eval, 0) adds a new dimension at the position 0.\n",
        "This transforms the data from a list to a more complex structure (a tensor) with an extra dimension.\n",
        "Examples:\n",
        "Example 1: Basic List\n",
        "Original List:\n",
        "\n",
        "input_eval = [2, 5, 7, 1]\n",
        "This is a simple list of numbers.\n",
        "Adding Dimension:\n",
        "\n",
        "input_eval = tf.expand_dims(input_eval, 0)\n",
        "Now, input_eval becomes [[2, 5, 7, 1]].\n",
        "Here, the list has been nested inside another list, making it a 2D structure (like a matrix with one row).\n"
      ],
      "metadata": {
        "id": "PJeAyuZp9nyI"
      },
      "id": "PJeAyuZp9nyI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is tf.squeeze?\n",
        "tf.squeeze is a function that removes dimensions of size 1 from the shape of a tensor.\n",
        "\n",
        "Example of tf.squeeze:\n",
        "Input Tensor: Let's say you have a tensor with shape [1, 5, 3]. This means:\n",
        "\n",
        "1 batch of data\n",
        "Each batch has 5 items\n",
        "Each item has 3 features\n",
        "Using tf.squeeze:\n",
        "\n",
        "tf.squeeze(tensor, 0) removes the first dimension if it's size 1.\n",
        "The result tensor will have shape [5, 3].\n",
        "Applying it to predictions:\n",
        "Before Squeeze: predictions might have a shape like [1, num_chars] because model(input_eval) returns a batch of predictions, even if the batch size is 1.\n",
        "\n",
        "Example: If num_chars is 26 (for an alphabet), shape could be [1, 26].\n",
        "After Squeeze: tf.squeeze(predictions, 0) removes the batch dimension (size 1), resulting in a shape of [26].\n",
        "\n",
        "This makes it easier to work with the predictions directly.\n",
        "Why Squeeze?\n",
        "By removing the batch dimension, you get a simpler tensor that directly represents the predictions for the next character. This makes further operations easier."
      ],
      "metadata": {
        "id": "h9tZ4xvHDS_I"
      },
      "id": "h9tZ4xvHDS_I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example**\n",
        "**Hello**\n",
        "\n",
        "\n",
        " Let's go through a step-by-step example to see how predictions are created and how temperature affects them using the seed \"hello\".\n",
        "\n",
        "Step-by-Step Example\n",
        "Initial Setup:\n",
        "\n",
        "Seed: \"hello\"\n",
        "Model: Let's assume the model has been trained to predict the next character in a text.\n",
        "Vocabulary: Assume a simple vocabulary ['a', 'b', 'c', ..., 'z', ' ']\n",
        "Convert Seed to Indices:\n",
        "\n",
        "char_to_index: {'a': 0, 'b': 1, ..., 'z': 25, ' ': 26}\n",
        "index_to_char: {0: 'a', 1: 'b', ..., 25: 'z', 26: ' '}\n",
        "\"hello\" -> [7, 4, 11, 11, 14] (indices of 'h', 'e', 'l', 'l', 'o')\n",
        "Initial Predictions:\n",
        "\n",
        "Assume the model makes predictions for the next character and produces a probability distribution (logits) for each character.\n",
        "Example logits for simplicity (these would normally be output by the model): [2.0, 0.5, 0.1, ..., 1.5, 0.3] (27 values corresponding to each character)\n",
        "Temperature Adjustment\n",
        "Let's see how different temperatures would adjust these logits.\n",
        "\n",
        "1. Without Temperature Adjustment (Temperature = 1):\n",
        "Logits: [2.0, 0.5, 0.1, ..., 1.5, 0.3]\n",
        "2. High Temperature (e.g., Temperature = 2):\n",
        "Adjusted Logits: Each logit is divided by 2.\n",
        "Adjusted Logits: [1.0, 0.25, 0.05, ..., 0.75, 0.15]\n",
        "3. Low Temperature (e.g., Temperature = 0.5):\n",
        "Adjusted Logits: Each logit is divided by 0.5 (i.e., multiplied by 2).\n",
        "Adjusted Logits: [4.0, 1.0, 0.2, ..., 3.0, 0.6]\n",
        "4. Temperature = 0 (Special Case Handling):\n",
        "Instead of dividing by zero, we can assume the model always picks the highest probability.\n",
        "Logits: [2.0, 0.5, 0.1, ..., 1.5, 0.3]\n",
        "Max Logit: The highest value is 2.0 (corresponding to the first character in this example).\n",
        "Converting Logits to Probabilities\n",
        "Next, we convert these adjusted logits to probabilities using the softmax function.\n",
        "\n",
        "1. Temperature = 1 (Original Logits):\n",
        "Softmax: softmax([2.0, 0.5, 0.1, ..., 1.5, 0.3])\n",
        "Probabilities: [0.3, 0.05, 0.03, ..., 0.2, 0.04] (values are illustrative)\n",
        "2. High Temperature (2):\n",
        "Softmax: softmax([1.0, 0.25, 0.05, ..., 0.75, 0.15])\n",
        "Probabilities: [0.2, 0.07, 0.05, ..., 0.15, 0.06] (more even distribution)\n",
        "3. Low Temperature (0.5):\n",
        "Softmax: softmax([4.0, 1.0, 0.2, ..., 3.0, 0.6])\n",
        "Probabilities: [0.6, 0.05, 0.03, ..., 0.25, 0.04] (more peaked distribution)\n",
        "4. Temperature = 0 (Special Case):\n",
        "Probabilities: Always picks the highest probability character.\n",
        "Probabilities: [1.0, 0.0, 0.0, ..., 0.0, 0.0]\n",
        "Selecting the Next Character\n",
        "Based on the probabilities, the next character is selected.\n",
        "\n",
        "1. Temperature = 1:\n",
        "Probabilities: [0.3, 0.05, 0.03, ..., 0.2, 0.04]\n",
        "Selected Character: Likely 'a' (index 0), but could be others due to some randomness.\n",
        "2. High Temperature (2):\n",
        "Probabilities: [0.2, 0.07, 0.05, ..., 0.15, 0.06]\n",
        "Selected Character: More variety, less predictable.\n",
        "3. Low Temperature (0.5):\n",
        "Probabilities: [0.6, 0.05, 0.03, ..., 0.25, 0.04]\n",
        "Selected Character: Likely 'a' (index 0), more predictable.\n",
        "4. Temperature = 0:\n",
        "Probabilities: [1.0, 0.0, 0.0, ..., 0.0, 0.0]\n",
        "Selected Character: Always 'a' (index 0).\n",
        "Building the Generated Text\n",
        "Let's assume we generate text for a few steps.\n",
        "\n",
        "Initial Seed: \"hello\"\n",
        "Step 1:\n",
        "\n",
        "Seed: \"hello\"\n",
        "Next Character:\n",
        "Temp = 1: 'a' or any other with some randomness\n",
        "Temp = 2: More random character\n",
        "Temp = 0.5: 'a' or another common character\n",
        "Temp = 0: Always 'a'\n",
        "Text: \"helloa\"\n",
        "Step 2:\n",
        "\n",
        "Seed: \"helloa\"\n",
        "Next Character: Same process repeats, showing how different temperatures affect each step.\n",
        "By following these steps, you can see how temperature affects the randomness and predictability of the generated text."
      ],
      "metadata": {
        "id": "YFVanYAnGeDF"
      },
      "id": "YFVanYAnGeDF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Softmax?**\n",
        "\n",
        "What is Softmax?\n",
        "The softmax function is used to convert a vector of logits (raw model predictions) into probabilities. It squashes the output of each class to be between 0 and 1 and divides by the sum of all the outputs so that the probabilities sum to 1.\n",
        "\n",
        "Formula:\n",
        "For a vector\n",
        "𝑧\n",
        "z of logits, the softmax function is defined as:\n",
        "\n",
        "softmax\n",
        "(\n",
        "𝑧\n",
        "𝑖\n",
        ")\n",
        "=\n",
        "𝑒\n",
        "𝑧\n",
        "𝑖\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝐾\n",
        "𝑒\n",
        "𝑧\n",
        "𝑗\n",
        "softmax(z\n",
        "i\n",
        "​\n",
        " )=\n",
        "∑\n",
        "j=1\n",
        "K\n",
        "​\n",
        " e\n",
        "z\n",
        "j\n",
        "​\n",
        "\n",
        "\n",
        "e\n",
        "z\n",
        "i\n",
        "​\n",
        "\n",
        "\n",
        "​\n",
        "\n",
        "\n",
        "where\n",
        "𝑧\n",
        "𝑖\n",
        "z\n",
        "i\n",
        "​\n",
        "  is the logit for class\n",
        "𝑖\n",
        "i and\n",
        "𝐾\n",
        "K is the number of classes.\n",
        "\n",
        "Which Part of the Code Uses Softmax?\n",
        "In your code, the softmax operation is implicitly performed when using tf.random.categorical. This function expects logits as input and internally applies the softmax function to convert these logits into probabilities for sampling."
      ],
      "metadata": {
        "id": "sR3ml7gjGw1Y"
      },
      "id": "sR3ml7gjGw1Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the following code?**\n",
        "\n",
        "\n",
        " predicted_id=tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "This line of code selects the next character to generate based on the model's predictions. Here's how it works step by step:\n",
        "\n",
        "Input: predictions:\n",
        "\n",
        "predictions is a list of logits (raw model outputs) for each possible character. Logits are scores that represent how likely each character is to be the next one.\n",
        "Softmax and Sampling:\n",
        "\n",
        "The tf.random.categorical(predictions, num_samples=1) function:\n",
        "Applies the softmax function to the logits to convert them into probabilities. Higher logits correspond to higher probabilities.\n",
        "Samples one character from the resulting probability distribution.\n",
        "Select the Sampled Character:\n",
        "\n",
        "[-1, 0] is used to get the sampled character's index:\n",
        "[-1, 0] selects the last sample (though in this case, there's only one sample) and the first element from that sample.\n",
        "Convert to a NumPy Number:\n",
        "\n",
        ".numpy() converts the TensorFlow tensor to a NumPy number, making it easier to work with in the rest of the code.\n",
        "Step-by-Step with Examples\n",
        "Let's go through an example to clarify each step.\n",
        "\n",
        "1. Model Predictions:\n",
        "Assume predictions is a tensor with logits for each character:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "predictions = [2.0, 0.5, 0.1, 1.5, 0.3]\n",
        "2. Apply Softmax and Sample:\n",
        "The tf.random.categorical function will:\n",
        "\n",
        "Convert predictions to probabilities using softmax.\n",
        "Sample one index based on these probabilities.\n",
        "For example:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "softmax_probabilities = [0.42, 0.12, 0.08, 0.28, 0.10]  # Result of softmax\n",
        "sampled_index = tf.random.categorical([predictions], num_samples=1)\n",
        "# Let's assume the sampled index is 0 (based on the highest probability 0.42)\n",
        "sampled_index = [[0]]\n",
        "3. Select the Sampled Character's Index:\n",
        "Using [-1, 0] to get the index from the sample:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "predicted_id = sampled_index[-1, 0]  # This will be 0 in our example\n",
        "4. Convert to NumPy:\n",
        "Convert the index to a NumPy number:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "predicted_id = predicted_id.numpy()  # This will be 0\n",
        "Summary\n",
        "The line predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy() does the following:\n",
        "\n",
        "Converts the model's logits into probabilities.\n",
        "Samples one character's index based on these probabilities.\n",
        "Extracts this index.\n",
        "Converts it to a NumPy number for further use.\n",
        "This way, the model selects the next character in the text generation process based on the predicted probabilities."
      ],
      "metadata": {
        "id": "0INnhLZAHjKW"
      },
      "id": "0INnhLZAHjKW"
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model,\"juliet\", gen_size=1000))"
      ],
      "metadata": {
        "id": "A9aiJGXS9seD",
        "outputId": "99ec9ca3-d07d-4798-b003-0cc9fb855962",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "A9aiJGXS9seD",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "juliets,\n",
            "    And rate that Clarence was not prince at cause.\n",
            "  SERVANT. O, that cock,\n",
            "  Other bosom left their\n",
            "     and ingrateful epbation- Iago at Lieh's House;\n",
            "    Ratcliff, condemn'd with my thousand woman,\n",
            "    I know you carry him.                               Exeunt SUNalther ring he upon her love\n",
            "    To purgy vassal; a strain bounds lie wit\n",
            "    With sorrow's eyes. With those fresh parcal dismaltmen,\n",
            "    In diotenddy does it not go wish how out of\n",
            "    the story of the excreman to havor with her, four of his hope\n",
            "    taken, monster, chaste, a light week. If you think I give to these the prayer, under the\n",
            "    shoulder-house, and show the fool. For no the speech, devise especially\n",
            "    Stands at their helps; for I must to procure by shame,\n",
            "    Emoning underneath the noble grace,\n",
            "    And chaste his lipe. If we think men shall all\n",
            "    All of all expedition; importure like to hear,\n",
            "  onace, spoil incapable\n",
            "    And turn him up and drink untruments;\n",
            "    Sweet, when the fierce yeighbound will w\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OgdtHM8gJHvs"
      },
      "id": "OgdtHM8gJHvs",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}